{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DbpfyohEWx"
      },
      "source": [
        "<div align=center>\n",
        "\t\t\n",
        "<p></p>\n",
        "<p></p>\n",
        "<font size=5>\n",
        "به نام خدا\n",
        "<font/>\n",
        "<p></p>\n",
        "<p></p>\n",
        "<font color=green size=4>\n",
        "بازیابی پیشرفته اطلاعات - تمرین سوم\n",
        "<br/>\n",
        "بوستان و گلستان سعدی\n",
        "</font>\n",
        "<br/>\n",
        "<br/>\n",
        "<font size=3>\n",
        "بهار 1401\n",
        "<br/>\n",
        "<font/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "<hr/>\n",
        "<div align=center>\n",
        "\t\t    <font size=3>\n",
        "\t\t\t    <br />\n",
        "\t\t\t\tگروه 33:\n",
        "\t\t\t\tعلی مهربانی ، آرمان سلیمانی ، پانیذ حلواچی\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6mih4H0hEW4",
        "outputId": "a1f527e8-d8c4-45d8-8e25-81f872e8e95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.19.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "#import necessary libraries\n",
        "!pip install hazm\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import json\n",
        "from random import randint\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "import codecs\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import spatial\n",
        "from gensim.models import FastText\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDeCwWWbhEW5"
      },
      "source": [
        "<div align=center>\n",
        "\t\t    <font size=4 color=green>\n",
        "\t\t\t    <br />\n",
        "                بخش اول:\n",
        "                </font>\n",
        "                <font size=3>\n",
        "                دریافت داده‌ها\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>\n",
        "    <hr/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lpBF6TSQhEW6"
      },
      "outputs": [],
      "source": [
        "all_files = listdir('Boostan/')\n",
        "\n",
        "i = 0\n",
        "df = pd.DataFrame(columns=[0])\n",
        "\n",
        "for p in all_files:\n",
        "    f = open('Boostan/' + p, 'r', encoding='utf8')\n",
        "    \n",
        "    has_poems = True\n",
        "    \n",
        "    \n",
        "    while has_poems:\n",
        "        poem = ''\n",
        "        \n",
        "        while True:\n",
        "            s = f.readline()\n",
        "            if '$' in s:\n",
        "                has_poems = False\n",
        "                break\n",
        "            elif '_' in s:\n",
        "                break\n",
        "            poem = poem + s\n",
        "        \n",
        "        #inja ye poem kamel shode\n",
        "        if poem != '':\n",
        "            df.loc[i] = poem\n",
        "            i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY6Me7JFhEW7",
        "outputId": "8c4b11a4-5fb2-45a8-e878-e58a887b30e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNA0rl_hEW8"
      },
      "source": [
        "<div align=center>\n",
        "\t\t    <font size=4 color=green>\n",
        "\t\t\t    <br />\n",
        "                بخش دوم:\n",
        "                </font>\n",
        "                <font size=3>\n",
        "                پیش پردازش اولیه متن\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>\n",
        "    <hr/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ew0w_hhEW9",
        "outputId": "0055e5d0-2c6d-4804-b98b-59167b9b173c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "جوانی هنرمند فرزانه بود\n",
            "که در وعظ چالاک و مردانه بود\n",
            "نکونام و صاحبدل و حق پرست\n",
            "خط عارضش خوشتر از خط دست\n",
            "قوی در بلاغات و در نحو چست\n",
            "ولی حرف ابجد نگفتی درست\n",
            "یکی را بگفتم ز صاحبدلان\n",
            "که دندان پیشین ندارد فلان\n",
            "برآمد ز سودای من سرخ روی\n",
            "کز این جنس بیهوده دیگر مگوی\n",
            "تو در وی همان عیب دیدی که هست\n",
            "ز چندان هنر چشم عقلت ببست\n",
            "یقین بشنو از من که روز یقین\n",
            "نبینند بد، مردم نیک بین\n",
            "یکی را که فضل است و فرهنگ و رای\n",
            "گرش پای عصمت بخیزد ز جای\n",
            "به یک خرده مپسند بر وی جفا\n",
            "بزرگان چه گفتند؟ خذ ما صفا\n",
            "بود خار و گل با هم ای هوشمند\n",
            "چه در بند خاری؟ تو گل دسته بند\n",
            "که را زشت خویی بود در سرشت\n",
            "نبیند ز طاووس جز پای زشت\n",
            "صفایی به دست آور ای خیره روی\n",
            "که ننماید آیینهٔ تیره، روی\n",
            "طریقی طلب کز عقوبت رهی\n",
            "نه حرفی که انگشت بر وی نهی\n",
            "منه عیب خلق ای خردمند پیش\n",
            "که چشمت فرو دوزد از عیب خویش\n",
            "چرا دامن آلوده را حد زنم\n",
            "چو در خود شناسم که تردامنم؟\n",
            "نشاید که بر کس درشتی کنی\n",
            "چو خود را به تأویل پشتی کنی\n",
            "چو بد ناپسند آیدت خود مکن\n",
            "پس آنگه به همسایه گو بد مکن\n",
            "من ار حق شناسم وگر خود نمای\n",
            "برون با تو دارم، درون با خدای\n",
            "چو ظاهر به عفت بیاراستم\n",
            "تصرف مکن در کژ و راستم\n",
            "اگر سیرتم خوب و گر منکر است\n",
            "خدایم به سر از تو داناتر است\n",
            "تو خاموش اگر من بهم یا بدم\n",
            "که حمال سود و زیان خودم\n",
            "کسی را به کردار بد کن عذاب\n",
            "که چشم از تو دارد به نیکی ثواب\n",
            "نکو کاری از مردم نیک رای\n",
            "یکی را به ده می‌نویسد خدای\n",
            "تو نیز ای عجب هر که را یک هنر\n",
            "ببینی، ز ده عیبش اندر گذر\n",
            "نه یک عیب او را بر انگشت پیچ\n",
            "جهانی فضیلت بر آور به هیچ\n",
            "چو دشمن که در شعر سعدی، نگاه\n",
            "به نفرت کند ز اندرون تباه\n",
            "ندارد به صد نکتهٔ نغز گوش\n",
            "چو زحفی ببیند بر آرد خروش\n",
            "جز این علتش نیست کان بد پسند\n",
            "حسد دیده نیک بینش بکند\n",
            "نه مر خلق را صنع باری سرشت؟\n",
            "سیاه و سپید آمد و خوب و زشت\n",
            "نه هر چشم و ابرو که بینی نکوست\n",
            "بخور پسته مغز و بینداز پوست\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#copy df\n",
        "poems = df.copy()\n",
        "print(df.at[223, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-63M46AhEW-"
      },
      "source": [
        "### z/az  gah/gaah hazf kardane b az fe'l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aVJRZVtGhEW-"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "for i in range(0, df.shape[0]):\n",
        "    df.at[i, 0] = normalizer.normalize(df.at[i, 0]).replace(' ز ', ' از ').replace(' مر ', ' ') #normalize beshe va z beshe az\n",
        "    \n",
        "    words = word_tokenize(df.at[i, 0])\n",
        "    \n",
        "    for j in range(0, len(words)):\n",
        "        \n",
        "        word = words[j]\n",
        "        past_root = lemmatizer.lemmatize(word).split('#', 1)[0]\n",
        "        past_root = past_root.replace('آ', 'ا')\n",
        "        word_modified = word.replace(past_root, '')\n",
        "        \n",
        "        if word_modified != '':\n",
        "            if word_modified[0] == 'ب' and len(word_modified) < 4:\n",
        "                #yani fele maazi bode be forme ghadimi\n",
        "                df.at[i, 0] = df.at[i, 0].replace(word, word[1:])\n",
        "        \n",
        "        if word.endswith('گه'):\n",
        "            df.at[i, 0] = df.at[i, 0].replace(word, word[:-2]+'گاه')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pW0GYBihEW_"
      },
      "source": [
        "### fele amre bastani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J45rlhzehEXA"
      },
      "outputs": [],
      "source": [
        "for i in range(0, df.shape[0]):\n",
        "    \n",
        "    words = word_tokenize(df.at[i, 0])\n",
        "    \n",
        "    for j in range(0, len(words)):\n",
        "        \n",
        "        word = words[j]\n",
        "        \n",
        "        if word[0] == 'م':\n",
        "            word_modified = 'ب' + word[1:]\n",
        "            present_root = lemmatizer.lemmatize(word_modified).split('#', 1)[-1]\n",
        "            present_root = present_root.replace('آ', 'ا')\n",
        "            word_modified = word_modified.replace(present_root, '')\n",
        "            \n",
        "            if word_modified == 'ب':\n",
        "                \n",
        "                df.at[i, 0] = df.at[i, 0].replace(word, 'ن'+word[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPaNDXjEhEXB"
      },
      "source": [
        "### other preprocess reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D6MdEN8-hEXB"
      },
      "outputs": [],
      "source": [
        "with open('Exception.txt', encoding='utf8') as exc:\n",
        "    data = exc.read()\n",
        "special_verbs = json.loads(data)\n",
        "\n",
        "\n",
        "df_temp = pd.DataFrame(columns=[0])\n",
        "stopwords = ['!','،','؟','ز','ار']+[normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
        "\n",
        "for i in range(0, df.shape[0]):\n",
        "    \n",
        "    words = word_tokenize(df.at[i, 0])\n",
        "    words = [t for t in words if t not in stopwords]\n",
        "    lemmatized_tokens = []\n",
        "    \n",
        "    for t in words:\n",
        "        if t in special_verbs.keys():\n",
        "            lemmatized_tokens.append(t)\n",
        "        else:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(t).split('#')[0])\n",
        "    \n",
        "    lemmatized = ' '.join(lemmatized_tokens)\n",
        "            \n",
        "    df.at[i, 0] = lemmatized\n",
        "    df_temp.at[i, 0] = lemmatized_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NAD4WfnhEXD",
        "outputId": "6e60ba0f-85c4-47fb-a31b-32264ae0e36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['برآریم', 'دست', 'دل', 'توانست', 'برآورد', 'فردا', 'گل', 'فصل', 'خزان', 'دید', 'درخت', 'برگ', 'ماند', 'سرما', 'سخت', 'برآرد', 'تهی', 'دست', 'رحمت', 'گشت', 'تهیدست', 'مپندار', 'هرگز', 'بست', 'نومید', 'آورده', 'دست', 'قضا', 'خلعت', 'ناندارش', 'قدر', 'میوه', 'آستین', 'نهد', 'طاعت', 'آرند', 'مسکین', 'درگاه', 'مسکین', 'نواز', 'چو', 'شاخ', 'برهنه', 'برآریم', 'دست', 'برگ', 'توانست', 'خداوندگارا', 'کن', 'جود', 'جرم', 'بندگان', 'گناه', 'آمد', 'بنده', 'خاکسار', 'امید', 'عفو', 'خداوندگار', 'کریما', 'رزق', 'پرورده', 'انعام', 'لطف', 'خو', 'کرد', 'گدا', 'کرم', 'دید', 'لطف', 'ناز', 'گشت', 'دنبال', 'بخشنده', 'چو', 'دنیا', 'کردی', 'عزیز', 'عقب', 'چشم', 'عزیزی', 'خوار', 'بس', 'عزیز', 'خوار', 'دید', 'کس', 'خدایا', 'عزت', 'خوار', 'کرد', 'ذل', 'گنه', 'شرمسار', 'کرد', 'مسلط', 'کرد', 'منی', 'سر', 'دست', 'به\\u200cگر', 'عقوبت', 'رم', 'گیتی', 'بتر', 'زین', 'بود', 'بدی', 'جفا', 'بردن', 'دست', 'خودی', 'مرا', 'شرمساری', 'بس', 'دگر', 'شرمسار', 'کرد', 'کس', 'گرم', 'سر', 'افتاد', 'سایه', 'سپهر', 'که', 'پایه\\u200cای', 'تاج', 'سر', 'افرازدم', 'بردار', 'کس', 'نیندازدم', 'تنم', 'می\\u200cبلرزد', 'چو', 'یاد', 'آورد', 'مناجات', 'شوریده', 'حرم', 'گفت', 'شوریده', 'دلفگار', 'الها', 'بخشید', 'ذل', 'ندار', 'همی\\u200cگفت', 'حق', 'زاری', 'بس', 'میفکن', 'دستم', 'گرفت', 'لطف', 'خواند', 'مران', 'در', 'آستان', 'سر', 'دانست', 'مسکین', 'بیچاره', 'فرو', 'مانده', 'نفس', 'اماره', 'نمی\\u200cتازد', 'نفس', 'سرکش', 'چنان', 'عقل', 'توانست', 'گرفتن', 'عنان', 'نفس', 'شیطان', 'آمد', 'زور', 'مصاف', 'پلنگ', 'آمد', 'مور', 'مرد', 'راه', 'راه', 'ده', 'وز', 'دشمنانم', 'پناهی', 'ده', 'خدایا', 'ذات', 'خداوند', 'اوصاف', 'مانند', 'لبیک', 'حجاج', 'بیت\\u200cالحرام', 'مدفون', 'یثرب', 'علیه\\u200cالسلام', 'تکبیر', 'مرد', 'شمشیر', 'زن', 'مرد', 'وغا', 'شمرد', 'زن', 'طاعات', 'پیر', 'آراسته', 'صدق', 'جوان', 'نوخاسته', 'ورطه', 'نفس', 'ننگ', 'گفتن', 'فریاد', 'رس', 'امید', 'طاعت', 'طاعتان', 'شفاعت', 'پاک', 'کز', 'آلایشم', 'وگر', 'زلتی', 'معذور', 'پیر', 'عبادت', 'شرم', 'گنه', 'دیده', 'پا', 'چشم', 'سعادت', 'بست', 'زبان', 'وقت', 'شهادت', 'بست', 'چراغ', 'یقین', 'فرا', 'بد', 'کردن', 'دست', 'کوتاه', 'بگردان', 'نادیدن', 'دید', 'داد', 'دست', 'ناپسندیده', 'ذره', 'هوا', 'احتقارم', 'خورشید', 'لطف', 'شعاع', 'بس', 'شعاع', 'دید', 'کس', 'بدی', 'نگاه', 'کن', 'کس', 'گدا', 'شاه', 'التفات', 'بس', 'مرا\\u200cگر', 'گرفت', 'انصاف', 'بنالم', 'عفو', 'وعده', 'خدایا', 'ذلت', 'مران', 'در', 'صورت', 'بست', 'دری', 'دیگر', 'ور', 'جهل', 'غایب', 'شد', 'روز', 'کنون', 'کآمدم', 'رفت', 'بست', 'عذر', 'آرم', 'ننگ', 'تردامنی', 'مگر', 'عجز', 'آورد', 'کای', 'غنی', 'فقیر', 'جرم', 'گناه', 'گرفت', 'غنی', 'ترحم', 'فقیر', 'ضعف', 'حال', 'گریس', 'ضعیف', 'پناه', 'قوی', 'خدایا', 'غفلت', 'شکست', 'عهد', 'زور', 'قضا', 'دست', 'جهد', 'برخیزد', 'دست', 'تدبیر', 'نکته', 'بس', 'عذر', 'تقصیر', 'زد', 'قوت', 'خدا', 'خودی', 'سر', 'حکمت', 'می\\u200cرم', 'حکمت', 'سر']\n"
          ]
        }
      ],
      "source": [
        "print(df_temp.at[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfmEbrchEXD"
      },
      "source": [
        "<div align=center>\n",
        "\t\t    <font size=4 color=green>\n",
        "\t\t\t    <br />\n",
        "                بخش سوم:\n",
        "                </font>\n",
        "                <font size=3>\n",
        "                پیشنهاد شعر\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>\n",
        "    <hr/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#helper functions\n",
        "k = 2\n",
        "\n",
        "#return index of 10 largest elements\n",
        "def ten_largest_elements(list1,k):\n",
        "  templist = list1.copy()\n",
        "  index_list = []\n",
        "  for i in range(k):\n",
        "    index, largest_values = largest_element_index(templist)\n",
        "    index_list.append(index)\n",
        "    templist[index] = 0\n",
        "  return index_list\n",
        "\n",
        "#return the largest element\n",
        "def largest_element_index(temp_list):\n",
        "  largest_value = 0\n",
        "  index = 0\n",
        "  for i,element in enumerate(temp_list):\n",
        "    if element>largest_value:\n",
        "      largest_value = element\n",
        "      index = i\n",
        "  return (index,largest_value)\n",
        "\n",
        "#print poems with mentioned indices\n",
        "def print_poems(list_of_indices):\n",
        "  for i, index in enumerate(list_of_indices):\n",
        "    print(\"rank \",i+1,\":\")\n",
        "    print(poems.at[index,0],\"\\n\")"
      ],
      "metadata": {
        "id": "DJS2Tn4sUK-a"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkglrFkLhEXE"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\t\t    <font size=2>\n",
        "\t\t\t      1- استفاده از روش boolean\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(itertools.chain(*df.values))\n",
        "number_of_documents = len(documents)"
      ],
      "metadata": {
        "id": "QFyoO5UH8mt0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2BmOKXfNhEXE"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(documents)\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "#unique words\n",
        "terms = list(vocabulary.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#return boolean vector representation \n",
        "def bool_Representation(documents):\n",
        "  matrix = []\n",
        "  for document in documents:\n",
        "    dummy_list = []\n",
        "    for j in terms:\n",
        "      if j in document:\n",
        "        dummy_list.append(1)\n",
        "      else:\n",
        "        dummy_list.append(0)\n",
        "    matrix.append(dummy_list)\n",
        "  return matrix\n",
        "\n",
        "#return jaccard similarity between two sets\n",
        "def jaccard_similarity(A, B):\n",
        "    nominator = A.intersection(B)\n",
        "    denominator = A.union(B)\n",
        "    similarity = len(nominator)/len(denominator)\n",
        "    return similarity\n",
        "\n",
        "#return number of similar elements between query and each document\n",
        "def prediction(query_vector,matrix):\n",
        "    scores = []\n",
        "    count = 0\n",
        "    term_len = len(terms)\n",
        "    for i, vector in enumerate(matrix):\n",
        "      #we can change to sum of the query_vector and vector)\n",
        "        for t in range(term_len):\n",
        "            if(query_vector[t] == vector[t]):\n",
        "                count += 1\n",
        "        scores.append(count)\n",
        "        count = 0\n",
        "    return scores"
      ],
      "metadata": {
        "id": "qWyrSLZc_4R3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_temp = bool_Representation(documents)\n",
        "def boolean_method(input):\n",
        "  # desired query [does not need a tokenization]\n",
        "  query = bool_Representation([input])[0]\n",
        "  ###########\n",
        "  scores = prediction(query,matrix_temp)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "D3loe0a8AV0I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\t\t    <font size=2>\n",
        "\t\t\t      2- استفاده از tf-idf\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "AYHDJY0siwey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(itertools.chain(*df.values))\n",
        "number_of_documents = len(documents)"
      ],
      "metadata": {
        "id": "RMBEB06rDsyi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "def tf_idf_method(input):\n",
        "  # fit each time \n",
        "  tfidf_vectorizer.fit(documents)\n",
        "  tfidf_matrix = tfidf_vectorizer.transform(documents).todense().tolist()\n",
        "  #desired query [does not need tokenization]\n",
        "  query = [input]\n",
        "  #################\n",
        "  query_vector = tfidf_vectorizer.transform(query).todense().tolist()[0]\n",
        "  #calculate similarity\n",
        "  similarities = []\n",
        "  for tf_idf_vector in tfidf_matrix:\n",
        "    similarity = 1 - spatial.distance.cosine(query_vector,tf_idf_vector)\n",
        "    similarities.append(similarity)\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "HVn9vt1Zizlr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\t\t    <font size=2>\n",
        "\t\t\t      3- استفاده از روش fasttext\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "VG6OMn9ri0ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set hyperparameters\n",
        "embedding_size = 60\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2\n",
        "word_tokenized_corpus = list(itertools.chain(*df_temp.values))\n",
        "word_tokenizad_temp = word_tokenized_corpus.copy()"
      ],
      "metadata": {
        "id": "-QoXNaDYi4K_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model and train it\n",
        "model = FastText(size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling)\n",
        "model.build_vocab(word_tokenized_corpus)\n",
        "model.train(word_tokenized_corpus, total_examples=len(word_tokenized_corpus), epochs=100)"
      ],
      "metadata": {
        "id": "O3UQW4zni6xO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate mean vector for each poem\n",
        "poem_vectors=[[]] * len(word_tokenizad_temp)\n",
        "for i,poem in enumerate(word_tokenizad_temp):\n",
        "  poem_vectors[i] = [0 for i in range(embedding_size)]\n",
        "  length = len(poem)\n",
        "  for word in poem:\n",
        "    try:\n",
        "        temp = model.wv[word]\n",
        "        vector = poem_vectors[i]\n",
        "        poem_vectors[i] = [vector[j]+temp[j] for j in range(embedding_size)]\n",
        "        break\n",
        "    except:\n",
        "        length -= 1\n",
        "  poem_vectors[i] = [item / length for item in poem_vectors[i]]"
      ],
      "metadata": {
        "id": "8YD6iqJ-rr8i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_method(input):\n",
        "  # desired query [tokenization is needed]\n",
        "  query = input\n",
        "  tokenized_query = word_tokenize(query)\n",
        "  ##############\n",
        "  query_vector = [0 for i in range(embedding_size)]\n",
        "  length = len(tokenized_query)\n",
        "  #calculate mean of query's words vectors\n",
        "  for word1 in tokenized_query:\n",
        "    try:\n",
        "          temp1 = model.wv[word1]\n",
        "          query_vector = [query_vector[j]+temp1[j] for j in range(embedding_size)]\n",
        "          break\n",
        "    except:\n",
        "          length -= 1\n",
        "  query_vector = [item / length for item in query_vector]\n",
        "  #calculate cosine similarity\n",
        "  similarities = []\n",
        "  for poem_vector in poem_vectors:\n",
        "    similarity = 1 - spatial.distance.cosine(query_vector,poem_vector)\n",
        "    similarities.append(similarity)\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "HCYik9CxzxKk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\t\t    <font size=2>\n",
        "\t\t\t      4- استفاده از روش bert\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "ILuG6vZYd8rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_bert = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "MiNvuBM_eCkU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = list(itertools.chain(*df.values))\n",
        "number_of_documents = len(documents)\n",
        "def bert_method(input):\n",
        "  # desired query [tokenization is not needed]\n",
        "  query = input\n",
        "  ###########\n",
        "  documents.append(query)\n",
        "  sentence_embeddings = model_bert.encode(documents)\n",
        "  similarities = list(itertools.chain(*cosine_similarity(\n",
        "    [sentence_embeddings[-1]],\n",
        "    sentence_embeddings[0:-2]\n",
        "    )))\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "9EIjFhuje4nk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=center>\n",
        "\t\t    <font size=4 color=green>\n",
        "\t\t\t    <br />\n",
        "                بخش نهایی:\n",
        "                </font>\n",
        "                <font size=3>\n",
        "                پیشنهاد شعر براساس ورودی داده شده\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "    <br/>\n",
        "    </div>\n",
        "    <hr/>"
      ],
      "metadata": {
        "id": "pUbV-Kenp_qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = input()\n",
        "method = input(\"Enter your desired method: \")\n",
        "if method == \"bert\":\n",
        "  print_poems(ten_largest_elements(bert_method(input_string),k))\n",
        "elif method == \"boolean\":\n",
        "  print_poems(ten_largest_elements(boolean_method(input_string),k))\n",
        "elif method == \"tf_idf\":\n",
        "  print_poems(ten_largest_elements(tf_idf_method(input_string),k))\n",
        "else:\n",
        "  print_poems(ten_largest_elements(fasttext_method(input_string),k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHC_VRZvp_Bo",
        "outputId": "38b5291b-3b4f-4dde-84ef-ec85b5e5715d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "شنیدم که در وقت نزع روان به هرمز چنین گفت نوشیروان که خاطر نگهدار درویش باش نه در بند آسایش خویش باش نیاساید اندر دیار تو کس چو آسایش خویش جویی و بس نیاید به نزدیک دانا پسند شبان خفته و گرگ در گوسفند برو پاس درویش محتاج دار که شاه از رعیت بود تاجدار رعیت چو بیخند و سلطان درخت درخت، ای پسر، باشد از بیخ سخت مکن تا توانی دل خلق ریش وگر می‌کنی می‌کنی بیخ خویش اگر جاده‌ای بایدت مستقیم ره پارسایان امید است و بیم طبیعت شود مرد را بخردی به امید نیکی و بیم بدی گر این هر دو در پادشه یافتی در اقلیم و ملکش پنه یافتی که بخشایش آرد بر امیدوار به امید بخشایش کردگار گزند کسانش نیاید پسند که ترسد که در ملکش آید گزند وگر در سرشت وی این خوی نیست در آن کشور آسودگی بوی نیست اگر پای بندی رضا پیش گیر وگر یک سواری سر خویش گیر فراخی در آن مرز و کشور مخواه که دلتنگ بینی رعیت ز شاه ز مستکبران دلاور بترس از آن کاو نترسد ز داور بترس دگر کشور آباد بیند به خواب که دارد دل اهل کشور خراب خرابی و بدنامی آید ز جور رسد پیش بین این سخن را به غور رعیت نشاید به بیداد کشت که مر سلطنت را پناهند و پشت مراعات دهقان کن از بهر خویش که مزدور خوشدل کند کار بیش مروت نباشد بدی با کسی کز او نیکویی دیده باشی بسی شنیدم که خسرو به شیرویه گفت در آن دم که چشمش زدیدن بخفت بر آن باش تا هرچه نیت کنی نظر در صلاح رعیت کنی الا تا نپیچی سر از عدل و رای که مردم ز دستت نپیچند پای گریزد رعیت ز بیدادگر کند نام زشتش به گیتی سمر بسی بر نیاید که بنیاد خود بکند آن که بنهاد بنیاد بد خرابی کند مرد شمشیر زن نه چندان که دود دل طفل و زن چراغی که بیوه زنی برفروخت بسی دیده باشی که شهری بسوخت از آن بهره‌ورتر در آفاق کیست که در ملکرانی به انصاف زیست چو نوبت رسد زین جهان غربتش ترحم فرستند بر تربتش بد و نیک مردم چو می‌بگذرند همان به که نامت به نیکی برند خداترس را بر رعیت گمار که معمار ملک است پرهیزگار\n",
            "Enter your desired method: fasttext\n",
            "rank  1 :\n",
            "شنیدم که مغروری از کبر مست\n",
            "در خانه بر روی سائل ببست\n",
            "به کنجی فرو ماند و بنشست مرد\n",
            "جگر گرم و آه از تف سینه سرد\n",
            "شنیدش یکی مرد پوشیده چشم\n",
            "بپرسیدش از موجب کین و خشم\n",
            "فرو گفت و بگریست بر خاک کوی\n",
            "جفایی کز آن شخصش آمد به روی\n",
            "بگفت ای فلان ترک آزار کن\n",
            "یک امشب به نزد من افطار کن\n",
            "به خلق و فریبش گریبان کشید\n",
            "به خانه در آوردش و خوان کشید\n",
            "بر آسود درویش روشن نهاد\n",
            "بگفت ایزدت روشنایی دهاد\n",
            "شب از نرگسش قطره چندی چکید\n",
            "سحر دیده بر کرد و دنیا بدید\n",
            "حکایت به شهر اندر افتاد و جوش\n",
            "که آن بی بصر دیده بر کرد دوش\n",
            "شنید این سخن خواجه سنگدل\n",
            "که برگشت درویش از او تنگدل\n",
            "بگفتا حکایت کن ای نیکبخت\n",
            "که چون سهل شد بر تو این کار سخت؟\n",
            "که بر کردت این شمع گیتی فروز؟\n",
            "بگفت ای ستمکار آشفته روز\n",
            "تو کوته نظر بودی و سست رای\n",
            "که مشغول گشتی به جغد از همای\n",
            "به روی من این در کسی کرد باز\n",
            "که کردی تو بر روی وی در، فراز\n",
            "اگر بوسه بر خاک مردان زنی\n",
            "به مردی که پیش آیدت روشنی\n",
            "کسانی که پوشیده چشم دلند\n",
            "همانا کز این توتیا غافلند\n",
            "چو برگشته دولت ملامت شنید\n",
            "سر انگشت حیرت به دندان گزید\n",
            "که شهباز من صید دام تو شد\n",
            "مرا بود دولت به نام تو شد\n",
            "کسی چون به دست آورد جره باز\n",
            "فرو برده چون موش دندان آز؟\n",
            "الا گر طلبکار اهل دلی\n",
            "ز خدمت مکن یک زمان غافلی\n",
            "خورش ده به گنجشک و کبک و حمام\n",
            "که یک روزت افتد همایی به دام\n",
            "چو هر گوشه تیر نیاز افکنی\n",
            "امید است ناگه که صیدی زنی\n",
            "دری هم بر آید ز چندین صدف\n",
            "ز صد چوبه آید یکی بر هدف\n",
            " \n",
            "\n",
            "rank  2 :\n",
            "شنیدم ز پیران شیرین سخن\n",
            "که بود اندر این شهر پیری کهن\n",
            "بسی دیده شاهان و دوران و امر\n",
            "سرآورده عمری ز تاریخ عمرو\n",
            "درخت کهن میوه‌ای تازه داشت\n",
            "که شهر از نکویی پرآوازه داشت\n",
            "عجب در زنخدان آن دل فریب\n",
            "که هرگز نبوده‌ست بر سرو سیب\n",
            "ز شوخی و مردم خراشیدنش\n",
            "فرج دید در سر تراشیدنش\n",
            "به موسی، کهن عمر کوته امید\n",
            "سرش کرد چون دست موسی سپید\n",
            "ز سر تیزی آن آهنین دل که بود\n",
            "به عیب پری‌رخ زبان برگشود\n",
            "به مویی که کرد از نکوییش کم\n",
            "نهادند حالی سرش در شکم\n",
            "چو چنگ از خجالت سر خوبروی\n",
            "نگونسار و در پیشش افتاده موی\n",
            "یکی را که خاطر در او رفته بود\n",
            "چو چشمان دلبندش آشفته بود\n",
            "کسی گفت جور آزمودی و درد\n",
            "دگر گرد سودای باطل مگرد\n",
            "ز مهرش بگردان چو پروانه پشت\n",
            "که مقراض، شمع جمالش بکشت\n",
            "برآمد خروش از هوادار چست\n",
            "که تردامنان را بود عهد سست\n",
            "پسر خوش منش باید و خوبروی\n",
            "پدر گو به جهلش بینداز موی\n",
            "مرا جان به مهرش برآمیخته‌ست\n",
            "نه خاطر به مویی در آویخته‌ست\n",
            "چو روی نکو داری انده مخور\n",
            "که موی ار بیفتد بروید دگر\n",
            "نه پیوسته رز خوشهٔ تر دهد\n",
            "گهی برگ ریزد، گهی بر دهد\n",
            "بزرگان چو خور در حجاب اوفتند\n",
            "حسودان چو اخگر در آب اوفتند\n",
            "برون آید از زیر ابر آفتاب\n",
            "به تدریج و اخگر بمیرد در آب\n",
            "ز ظلمت مترس ای پسندیده دوست\n",
            "که ممکن بود کاب حیوان در اوست\n",
            "نه گیتی پس از جنبش آرام یافت؟\n",
            "نه سعدی سفر کرد تا کام یافت؟\n",
            "دل از بی مرادی به فکرت مسوز\n",
            "شب آبستن است ای برادر به روز\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dhjs0RPhynaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "poem generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}